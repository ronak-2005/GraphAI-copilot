{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7af2944b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cb9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c21b91d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"../ml/moretoo/Ecommerce/train/df_Customers.csv\"\n",
    "OUTPUT_PATH = \"../ml/moretoo/Ecommerce/train/cleaned_Customers.csv\"\n",
    "CHUNK_SIZE = 10000\n",
    "MISSING_THRESHOLD = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922540f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset analyzed: 89,316 rows, 4 columns\n",
      " Total chunks: 9\n"
     ]
    }
   ],
   "source": [
    "col_missing_counts = Counter()\n",
    "col_unique_ratios = {}\n",
    "row_count = 0\n",
    "total_chunks = 0\n",
    "\n",
    "global_numeric_stats = defaultdict(list)\n",
    "global_categorical_stats = defaultdict(Counter)\n",
    "\n",
    "for chunk in pd.read_csv(FILE_PATH, chunksize=CHUNK_SIZE):\n",
    "    row_count += len(chunk)\n",
    "    total_chunks += 1\n",
    "    \n",
    "    col_missing_counts.update(chunk.isnull().sum().to_dict())\n",
    "    \n",
    "    for col in chunk.columns:\n",
    "        unique_count = chunk[col].nunique()\n",
    "        col_unique_ratios[col] = unique_count / len(chunk)\n",
    "    \n",
    "    if total_chunks <= 3:  \n",
    "        numeric_cols = chunk.select_dtypes(include=\"number\").columns\n",
    "        categorical_cols = chunk.select_dtypes(include=\"object\").columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            non_null = chunk[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                sample_size = min(100, len(non_null))\n",
    "                global_numeric_stats[col].extend(non_null.sample(sample_size).tolist())\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            non_null = chunk[col].dropna()\n",
    "            if len(non_null) > 0:\n",
    "                sample_size = min(50, len(non_null))\n",
    "                global_categorical_stats[col].update(non_null.sample(sample_size))\n",
    "\n",
    "print(f\"Dataset analyzed: {row_count:,} rows, {len(col_missing_counts)} columns\")\n",
    "print(f\" Total chunks: {total_chunks}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c0e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Smart column analysis...\n",
      " Column decisions:\n",
      " Dropping: 0 columns\n",
      " Keeping: 4 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ§  Smart column analysis...\")\n",
    "\n",
    "cols_to_drop = []\n",
    "keep_reasons = {}\n",
    "\n",
    "for col, missing_count in col_missing_counts.items():\n",
    "    missing_ratio = missing_count / row_count\n",
    "    unique_ratio = col_unique_ratios.get(col, 0)\n",
    "    col_lower = col.lower()\n",
    "    \n",
    "    if missing_ratio >= MISSING_THRESHOLD:\n",
    "        if any(keyword in col_lower for keyword in BUSINESS_KEYWORDS):\n",
    "            keep_reasons[col] = f\"Business metric (despite {missing_ratio:.1%} missing)\"\n",
    "            continue\n",
    "        \n",
    "        if unique_ratio > 0.8 and any(id_word in col_lower for id_word in ['id', 'key', 'code', 'ref']):\n",
    "            keep_reasons[col] = f\"Likely identifier (despite {missing_ratio:.1%} missing)\"\n",
    "            continue\n",
    "        \n",
    "\n",
    "        cols_to_drop.append(col)\n",
    "    else:\n",
    "        keep_reasons[col] = f\"Good quality ({missing_ratio:.1%} missing)\"\n",
    "\n",
    "print(f\" Column decisions:\")\n",
    "print(f\" Dropping: {len(cols_to_drop)} columns\")\n",
    "print(f\" Keeping: {len(keep_reasons)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d4ffdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples of column decisions:\n",
      " DROPPING:\n",
      "KEEPING:\n",
      "   1. customer_id                    (Good quality (0.0% missing))\n",
      "   2. customer_zip_code_prefix       (Good quality (0.0% missing))\n",
      "   3. customer_city                  (Good quality (0.0% missing))\n",
      "   4. customer_state                 (Good quality (0.0% missing))\n"
     ]
    }
   ],
   "source": [
    "print(\"Examples of column decisions:\")\n",
    "print(\" DROPPING:\")\n",
    "for i, col in enumerate(cols_to_drop[:5]):\n",
    "    missing_ratio = col_missing_counts[col] / row_count\n",
    "    print(f\"   {i+1}. {col:<30} ({missing_ratio:.1%} missing)\")\n",
    "\n",
    "print(\"KEEPING:\")\n",
    "for i, (col, reason) in enumerate(list(keep_reasons.items())[:5]):\n",
    "    print(f\"   {i+1}. {col:<30} ({reason})\")\n",
    "\n",
    "if len(cols_to_drop) > 5:\n",
    "    print(f\"\\n... and {len(cols_to_drop) - 5} more columns to drop\")\n",
    "if len(keep_reasons) > 5:\n",
    "    print(f\"... and {len(keep_reasons) - 5} more columns to keep\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2d64b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Global fallback values calculated for 4 columns\n",
      " Sample fallback values:\n",
      "   customer_zip_code_prefix: 35630.0\n",
      "   customer_id: rPgpYIdxiO5G\n",
      "   customer_city: sao paulo\n",
      "   customer_state: SP\n"
     ]
    }
   ],
   "source": [
    "global_fallbacks = {}\n",
    "\n",
    "for col, values in global_numeric_stats.items():\n",
    "    if len(values) > 0:\n",
    "        global_fallbacks[col] = pd.Series(values).median()\n",
    "\n",
    "for col, counter in global_categorical_stats.items():\n",
    "    if len(counter) > 0:\n",
    "        global_fallbacks[col] = counter.most_common(1)[0][0]\n",
    "\n",
    "print(f\" Global fallback values calculated for {len(global_fallbacks)} columns\")\n",
    "print(\" Sample fallback values:\")\n",
    "for i, (col, val) in enumerate(list(global_fallbacks.items())[:5]):\n",
    "    print(f\"   {col}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe0aca0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Local imputation function defined\n"
     ]
    }
   ],
   "source": [
    "def smart_local_impute(chunk, global_fallbacks):\n",
    "    numeric_cols = chunk.select_dtypes(include='number').columns\n",
    "    categorical_cols = chunk.select_dtypes(include='object').columns\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if chunk[col].isnull().any():\n",
    "            non_null = chunk[col].dropna()\n",
    "            \n",
    "            if len(non_null) >= 5:  \n",
    "                fill_value = non_null.median()\n",
    "            else:\n",
    "                fill_value = global_fallbacks.get(col, 0)\n",
    "            \n",
    "            chunk[col].fillna(fill_value, inplace=True)\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        if chunk[col].isnull().any():\n",
    "            non_null = chunk[col].dropna()\n",
    "            \n",
    "            if len(non_null) >= 3:  \n",
    "                mode_series = non_null.mode()\n",
    "                fill_value = mode_series[0] if not mode_series.empty else \"Unknown\"\n",
    "            else:\n",
    "                fill_value = global_fallbacks.get(col, \"Unknown\")\n",
    "            \n",
    "            chunk[col].fillna(fill_value, inplace=True)\n",
    "    \n",
    "    return chunk\n",
    "\n",
    "print(\" Local imputation function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e87042c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”„ Pass 2: Processing 9 chunks with LOCAL imputation...\n",
      " Chunk   1: 10,000 â†’ 10,000 rows\n",
      " Chunk   2: 10,000 â†’ 10,000 rows\n",
      " Chunk   3: 10,000 â†’ 10,000 rows\n",
      " Chunk   4: 10,000 â†’ 10,000 rows\n",
      " Chunk   5: 10,000 â†’ 10,000 rows\n",
      " All 9 chunks processed!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nðŸ”„ Pass 2: Processing {total_chunks} chunks with LOCAL imputation...\")\n",
    "\n",
    "Path(OUTPUT_PATH).unlink(missing_ok=True)\n",
    "\n",
    "first_write = True\n",
    "total_output_rows = 0\n",
    "chunk_stats = []\n",
    "\n",
    "for chunk_num, chunk in enumerate(pd.read_csv(FILE_PATH, chunksize=CHUNK_SIZE), 1):\n",
    "    \n",
    "    original_shape = chunk.shape\n",
    "    \n",
    "    chunk_clean = chunk.drop(columns=cols_to_drop, errors='ignore')\n",
    "    \n",
    "    row_missing_ratio = chunk_clean.isnull().mean(axis=1)\n",
    "    chunk_clean = chunk_clean[row_missing_ratio < 0.95]\n",
    "    \n",
    "    chunk_clean = smart_local_impute(chunk_clean, global_fallbacks)\n",
    "    chunk_clean = chunk_clean.drop_duplicates()\n",
    "    \n",
    "    if first_write:\n",
    "        chunk_clean.to_csv(OUTPUT_PATH, index=False, mode='w')\n",
    "        first_write = False\n",
    "    else:\n",
    "        chunk_clean.to_csv(OUTPUT_PATH, index=False, mode='a', header=False)\n",
    "    \n",
    "    total_output_rows += len(chunk_clean)\n",
    "    chunk_stats.append({\n",
    "        'chunk': chunk_num,\n",
    "        'original_rows': original_shape[0],\n",
    "        'final_rows': len(chunk_clean),\n",
    "        'row_reduction': (original_shape[0] - len(chunk_clean)) / original_shape[0]\n",
    "    })\n",
    "    \n",
    "    if chunk_num % 20 == 0 or chunk_num <= 5:\n",
    "        print(f\" Chunk {chunk_num:3d}: {original_shape[0]:,} â†’ {len(chunk_clean):,} rows\")\n",
    "\n",
    "print(f\" All {total_chunks} chunks processed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e7c2cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ‰ SUCCESS! Smart cleaning completed:\n",
      "==================================================\n",
      " ROWS:\n",
      "   Original: 89,316\n",
      "   Final:    89,316\n",
      "   Reduction: 0.0%\n",
      "\n",
      " COLUMNS:\n",
      "   Original: 4\n",
      "   Final:    4\n",
      "   Dropped:  0 (0.0%)\n",
      "\n",
      " OUTPUT: ../ml/moretoo/Ecommerce/train/cleaned_Customers.csv\n"
     ]
    }
   ],
   "source": [
    "final_df_sample = pd.read_csv(OUTPUT_PATH, nrows=1)\n",
    "final_cols = len(final_df_sample.columns)\n",
    "original_cols = len(col_missing_counts)\n",
    "\n",
    "total_original_rows = sum(stat['original_rows'] for stat in chunk_stats)\n",
    "row_reduction_pct = ((total_original_rows - total_output_rows) / total_original_rows * 100)\n",
    "col_reduction_pct = ((original_cols - final_cols) / original_cols * 100)\n",
    "\n",
    "print(\"ðŸŽ‰ SUCCESS! Smart cleaning completed:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\" ROWS:\")\n",
    "print(f\"   Original: {total_original_rows:,}\")\n",
    "print(f\"   Final:    {total_output_rows:,}\")\n",
    "print(f\"   Reduction: {row_reduction_pct:.1f}%\")\n",
    "print()\n",
    "print(f\" COLUMNS:\")\n",
    "print(f\"   Original: {original_cols}\")\n",
    "print(f\"   Final:    {final_cols}\")\n",
    "print(f\"   Dropped:  {len(cols_to_drop)} ({col_reduction_pct:.1f}%)\")\n",
    "print()\n",
    "print(f\" OUTPUT: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad229e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Quality Check - Sampling cleaned data:\n",
      " Sample Info:\n",
      "   Shape: (1000, 4)\n",
      "   Missing values: 0\n",
      "   Duplicates: 0\n",
      " Column types:\n",
      "   Numeric: 1\n",
      "   Categorical: 3\n",
      "   Datetime: 0\n",
      "Sample columns:\n",
      "    1. customer_id               (object, 0 nulls)\n",
      "    2. customer_zip_code_prefix  (int64, 0 nulls)\n",
      "    3. customer_city             (object, 0 nulls)\n",
      "    4. customer_state            (object, 0 nulls)\n",
      "\n",
      "ðŸš€ SIMPLE SMART CLEANING COMPLETE!\n",
      "ðŸ“ˆ Results:\n",
      "   â€¢ Processed 9 chunks\n",
      "   â€¢ Reduced rows by 0.0%\n",
      "   â€¢ Reduced columns by 0.0%\n",
      "   â€¢ Preserved 4 important columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Quality Check - Sampling cleaned data:\")\n",
    "\n",
    "sample_df = pd.read_csv(OUTPUT_PATH, nrows=1000)\n",
    "\n",
    "print(f\" Sample Info:\")\n",
    "print(f\"   Shape: {sample_df.shape}\")\n",
    "print(f\"   Missing values: {sample_df.isnull().sum().sum()}\")\n",
    "print(f\"   Duplicates: {sample_df.duplicated().sum()}\")\n",
    "\n",
    "print(f\" Column types:\")\n",
    "print(f\"   Numeric: {len(sample_df.select_dtypes(include='number').columns)}\")\n",
    "print(f\"   Categorical: {len(sample_df.select_dtypes(include='object').columns)}\")\n",
    "print(f\"   Datetime: {len(sample_df.select_dtypes(include='datetime').columns)}\")\n",
    "\n",
    "print(f\"Sample columns:\")\n",
    "for i, col in enumerate(sample_df.columns[:10]):\n",
    "    dtype = sample_df[col].dtype\n",
    "    null_count = sample_df[col].isnull().sum()\n",
    "    print(f\"   {i+1:2d}. {col:<25} ({dtype}, {null_count} nulls)\")\n",
    "\n",
    "if len(sample_df.columns) > 10:\n",
    "    print(f\"   ... and {len(sample_df.columns) - 10} more columns\")\n",
    "\n",
    "\n",
    "print(\"\\nðŸš€ SIMPLE SMART CLEANING COMPLETE!\")\n",
    "\n",
    "print(\"ðŸ“ˆ Results:\")\n",
    "print(f\"   â€¢ Processed {total_chunks} chunks\")\n",
    "print(f\"   â€¢ Reduced rows by {row_reduction_pct:.1f}%\")\n",
    "print(f\"   â€¢ Reduced columns by {col_reduction_pct:.1f}%\")\n",
    "print(f\"   â€¢ Preserved {len(keep_reasons)} important columns\")\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML Environment",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
